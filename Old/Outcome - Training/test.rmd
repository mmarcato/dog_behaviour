```{r, echo = FALSE}
# Adding a column to df vars for Test 
vars = vars %>% mutate(Test = case_when(
        Type == "ordinal" ~ "Logistic Regression",
        Type == "categorical" ~ "Chisq"))
print('Logistic Regression')
table(vars$Test)
unique(vars[vars$Test == 'Logistic Regression', 'Variable'])
print('Chisq')
unique(vars[vars$Test == 'Chisq', 'Variable'])

```

```{r, echo = FALSE}
# creating category other
levels(dc$Breed)[levels(dc$Breed) == "LRx"] <- "LRxGR"
levels(dc$Breed)[levels(dc$Breed) =="GR" | 
levels(dc$Breed) =="GS" | 
levels(dc$Breed) =="GRxPoodle"] <- "Other"
dc$Breed <- relevel(dc$Breed, "LR")

```


```{r, echo = FALSE}
# CATEGORICAL WITH CHISQ
dc1_cat_chi <- lapply(dc1_cat[-length(dc1_cat)], 
                            function(x) chisq.test(table(x, dc1_cat$Outcome)))
dc1_cat_chi_res <- lapply(dc1_cat_chi, function(x) c(x$statistic, x$parameter, x$p.value))
dc1_cat_chi_res <- data.frame(do.call(rbind, dc1_cat_chi_res))
colnames(dc1_cat_chi_res) <- c("X.squared", "df", "p_value")
dc1_cat_chi_res %>% filter(p_value < 0.2) %>% select(X.squared, df, p_value)
```

```{r, echo = FALSE}
# CATEGORICAL WITH CHISQ
dc2_cat_chi <- lapply(dc2_cat[-length(dc2_cat)], 
                            function(x) chisq.test(table(x, dc2_cat$Outcome)))
dc2_cat_chi_res <- lapply(dc2_cat_chi, function(x) c(x$statistic, x$parameter, x$p.value))
dc2_cat_chi_res <- data.frame(do.call(rbind, dc2_cat_chi_res))
colnames(dc2_cat_chi_res) <- c("X.squared", "df", "p_value")
dc2_cat_chi_res %>% filter(p_value < 0.2) %>% select(X.squared, df, p_value)
```
# Multivariate Logistic Regression using Caret
```{r, echo = FALSE}
# SIMPLE GLM
levels(data_reduced$Working)
model <- glm(Working ~ S.Distractions.Pull_mean+ S.Petting.Stimulus.During_mean + S.Sensitivity+ S.Crate.Stimulus_mean, 
    data = data_reduced, family = binomial(link = 'logit'))
summary(model)
#plot predictions
plot(predict(model, data_reduced, type = 'response'))

# train(Working ~ S.Distractions.Pull_mean + S.Sensitivity+ S.Crate.Stimulus_mean+ S.Petting.Stimulus.During_mean, dc1,
#             method = "lm", preProcess = ,
#             trControl = trainControl(method = "LOOCV")
#             )
```
# Caret RFE + preprocessing
RFE model in R. Apparently RFE allows you to specify pre-processing (link)[https://stackoverflow.com/questions/44776763/recursive-feature-elimination-with-caret-metric-roc-is-not-created-by-the-sum]. 
I could use the trControl = (trainControl)[https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/trainControl] and specify preProcOptions (link)[https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/preProcess]

```{r, echo = FALSE}
# Setting ROC as the metric for the Logistic Regression function
lrFuncs$summary <- twoClassSummary
set.seed(36)

ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                    method = "LOOCV",       # Leave One Out Cross Validation
                    # number = 1, 
                #     method = "repeatedcv",  # Repeated Cross Validation
                #     number = 4,             # Number of folds 
                #     repeats = 10,           # Number of iterations/repetition
                    verbose = FALSE,
                    returnResamp = "all")

# train control for preprocessing dataset
trainctrl <- trainControl(classProbs = TRUE, # compute the class probability (true enables ROC)
		        # preProcOptions = list(method = c('center', 'scale', 'knnImpute', 'pca'), k = 3, thresh = 0.95),
		        preProcOptions = list(k = 3, thresh = 0.95),
			summaryFunction = twoClassSummary # calculate ROC metric
)

# Recursive Feature Elimination with feat_lr
model <- rfe(Outcome ~ . ,                     # predict Outcome using all other variables
                data = dc_sign,                  # selecting the features from univariate lr
                sizes = 1:length(dc_sign), 
                # number = 1,                
                # Num_Resamples = 2,
                rfeControl = ctrl,
                # metric = "Accuracy")
                metric = "ROC",
                trControl = trainctrl,
                preProc = c('knnImpute', 'hello')) 

warnings()
print(model)
print(summary(model$fit))
predictors(model)

dim(dc_sign)
```


# Multivariable Logistic Regression using Recipes


```{r definition, echo = FALSE}

print("Dimensions of original dataset:")
print(dim(df))

model_recipe <- recipe(Outcome ~ ., data = dc1_sign)
summary(model_recipe)
model_steps <- model_recipe  %>%
                step_normalize(all_numeric()) %>%
                step_dummy(all_nominal(), -Outcome)
                # step_impute_mean(all_numeric()) %>%
                step_impute_knn(all_numeric(), neighbors = 3)  %>%

# prep(model_pca, training = df, retrain = TRUE) %>% 
# juice(all_predictors()) %>% 
# ncol()

# Setting ROC as the metric for the Logistic Regression function
lrFuncs$summary <- twoClassSummary
set.seed(36)    

ctrl <- rfeControl(functions = lrFuncs,     # logistic regression
                    # method = "LOOCV",       # Leave One Out Cross Validation
                    # number = 1, 
                    method = "repeatedcv",  # Repeated Cross Validation
                    number = 4,             # Number of folds 
                    repeats = 10,           # Number of iterations/repetition
                    verbose = FALSE,
                    returnResamp = "all")

# Recursive Feature Elimination with feat_lr
model <- rfe(model_pca,                     # predict Outcome using all other variables
                data = df,                  # selecting the features from univariate lr
                sizes = 1:length(df), 
                # number = 1,                
                # Num_Resamples = 2,
                rfeControl = ctrl,
                # metric = "Accuracy")
                metric = "ROC") 

# Printing outputs
warnings()
print(model)
print(summary(model$fit))
```


```{r, echo = FALSE}
## change the sign of the estimate if necessary
results_rfe <- as.data.frame(coef(summary(model_rfe$fit))) %>% mutate(Estimate  = -Estimate)
colnames(results_rfe)[2] <- "P-value"
results_rfe$OR <- exp(results_rfe$Estimate)
results_rfe <- cbind(results_rfe, exp(-confint(model_rfe$fit, level = 0.95)))
results_rfe <- results_rfe %>% select("Estimate", "P-value", "OR", "2.5 %"="97.5 %","97.5 %" = "2.5 %")
print(results_rfe)

## without changing the sign of the estimate
results_rfe <- as.data.frame(coef(summary(model_rfe$fit))) %>% select(1,4)
colnames(results_rfe)[2] <- "P-value"
results_rfe$OR <- exp(results_rfe$Estimate)
results_rfe <- cbind(results_rfe, exp(confint(model_rfe$fit, level = 0.95)))
print(results_rfe)


```