---
title: "Ethogram Data Collection 2 - Working (Success, Fail)"
author: "Marinara Marcato"
output: 
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "C:/Users/marinara.marcato/Project/Scripts/dog_ethogram")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
# install.packages("lmtest")
library(DescTools)
# plot
library(ggplot2)
library(ggpubr)
library(ggmosaic)
library(cowplot)
# datasets
library(plyr)
library(dplyr)
# library(conflicted) # prevent conflicts between packages
library(tidyverse)
library(reshape2) # remove?

library(ggsci)  # colors for the graphs - png colours for nature
library(broom)  # convert r output into tibbles
library(xtable) # latex table output
# machine learning/stats
library(recipes)
library(caret)
library(rstatix)
library(mlbench) # remove?
library(PRROC)  # calculates precision recall curve
library(lmtest) # lr test (log likelihood ratio test, nested models)
```

```{r output, include = FALSE, results = "hide", message = FALSE}
# save dataframes to csv and print LaTeX table
tab_results <- function(df, name, caption=NULL) {

    # remove the dots from the rownames
    rownames(df) <- gsub("[.]", " ", rownames(df))
    
    path = paste("3_results/dc2-working/", name, ".csv", sep = "")

    # save csv file
    write.csv(df, path)
    cat("dataframe saved to ", path)
    
    # latex code for table
#     print(xtable(df,
#         caption = paste(caption, ".", sep = ""),
#         label = paste("T-dc2-", name, sep = "")),
#         caption.placement = "top")
}

# save images as png and print LaTeX table
fig_results <- function(plot, name, caption = NULL, label = NULL){

    path = paste("3_results/dc2-working/", name, ".png", sep = "")
    
    # if label is not given, use name
    if (is.null(label)){
        label = name
    }
    # save png
    suppressMessages(ggsave(filename = path, 
                plot = plot + theme(text = element_text(size = 20)),
                width = 12, height = 8))
#     cat("Image saved at", path)

    # latex code for image
    # cat("\n\nStart LaTeX code\n\n",
    #         paste("\\begin{figure}[!h]\n\\centering\n\\caption{", paste(caption, ".", sep = ""),"}","\n\\label{F-dc2-", label, "}\n\\includegraphics[width = 11cm]{",path,"}\n\\end{figure}", sep = ""), 
    #         "\n\nEnd LaTeX Code\n\n")
}

model_or <- function(model){
    results <- as.data.frame(coef(summary(model))) %>% select(1,4)
    colnames(results)[2] <- "P-value"
    results$OR <- exp(results$Estimate)
    results <- cbind(results, exp(confint(model, level = 0.95)))
    return(results)
}

```


# Introduction 
This document shows the data analysis carried out to investigate the association between ethogram scores filled out by two trainers and the dog's training outcome.
Statistical methods will be used to test the hypothesis that there is a relationship between ethogram scores at dc2 and training outcome (Success, Fail). 


# Data Exploration
The dataset comprised of the following types and number of variables. The original ethogram items contained ordinal and categorical variables. 
The score variables were numerical derived from the original ethogram items.

```{r, echo = FALSE}
vars <- read.csv('0_data/0_raw/ethogram-variables-trainers.csv', stringsAsFactors=FALSE)
print(table(vars$Type))
ord <- vars %>% filter(Type == "ordinal") %>% pull("Variable")
cat <- vars %>% filter(Type == "categorical") %>% pull("Variable")

dc2 <- read.csv('0_data/2_prepare/2022-11-16_Ethogram-Trainers-DC2.csv', 
          stringsAsFactors=TRUE,        # imports character columns as factors
          na.strings=c("")              # imports "" as NA
        )

# changing data types
dc2$Data.Collection.Date <- as.Date(dc2$Data.Collection.Date, format= "%Y-%m-%d")
dc2$DOB = as.Date(dc2$DOB, format= "%Y-%m-%d")
dc2$DOA = as.Date(dc2$DOA, format= "%Y-%m-%d")
dc2$End.Date = as.Date(dc2$End.Date, format= "%Y-%m-%d")
dc2$Duration = as.numeric(gsub(" .*$", "", dc2$Duration))

dc2 <- dc2 %>% filter(Working != "Medical")
dc2$Working <- droplevels(dc2$Working)
dc2$Working <- relevel(dc2$Working, "Success")
```
All ordinal (N = 53) and categorical (N = 17) variables from the original ethogram items, 
score (N = 42) variables derived from the original ethogram items and 
demographical variables (N = 2) - namely, Sex and Breed - will be analysed.

## Demographics

Overview of the dogs participating in the data collection. 
Inclusion criteria: Breeds only Labrador Retrievers (LR) and Labrador Retrievers crossed with Golden Retrievers (LRxGR) were kept for data analysis.
```{r, echo = FALSE}
# removing dogs based on breed in the dc2 dataset
cat('Original dataset (dogs, variables): ', dim(dc2))
print(table(dc2$Breed))
levels(dc2$Breed)[levels(dc2$Breed) == "LRx"] <- "LRxGR"
# Changing Breeds
levels(dc2$Breed)[levels(dc2$Breed) == "GR"] <- "Other"
# dc2 <- dc2 %>% filter(Breed == "LRxGR" | Breed == "LR")
print(table(dc2$Breed))
dc2$Breed  <- droplevels(dc2$Breed)

cat('Final dataset (dogs, variables): ', dim(dc2))
print(table(dc2$Working))
print(prop.table(table(dc2$Working)))

# Sex
print(table(dc2$Sex))
```

Calculate descriptive statistics for age at arrival at the training centre for formal training. 
Calculate descriptive statistics for age at assessment when the dogs performed the behaviour test at data collection 1, which should be around week 3 of formal training.
```{r, echo = FALSE}
# n <- dim(dc2)[1]

dc2$Age.at.Arrival <- dc2$DOA - dc2$DOB
cat('Age at Arrival: Mean', round(mean(dc2$Age.at.Arrival)/30.417, 2), 
            'Standard Deviation', round(sd(dc2$Age.at.Arrival)/30.417, 2), 
            'Minimum:', round(min(dc2$Age.at.Arrival)/30.417, 2),
            'Maximum:', round(max(dc2$Age.at.Arrival)/30.417, 2))         
# mean <- mean(dc2$Age.at.Arrival)
# margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)
# 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)

dc2$Age.at.Assessment <- dc2$Data.Collection.Date - dc2$DOB
cat('Age at Assessment: Mean', round(mean(dc2$Age.at.Assessment)/30.417, 2), 
            'Standard Deviation', round(sd(dc2$Age.at.Assessment)/30.417, 2), 
            'Minimum:', round(min(dc2$Age.at.Assessment)/30.417, 2),
            'Maximum:', round(max(dc2$Age.at.Assessment)/30.417, 2))
# mean <- mean(dc2$Age.at.Assessment)
# margin <- qt(0.975,df=n-1)*sd(std)/sqrt(n)
# 'Confidence Interval', round((mean-margin)/30.417, 2), round((mean+margin)/30.417, 2)

```

Calculate statistics of duration of training for the dogs that were withdrawn from training. 
Performing this behaviour test at the beginning on the training (Week 3) would allow assistance dog training organisations to understand which dog are more suitable and allow them to make informed decisions when analysing which dogs to keep for training considering the results of this objective assessment. 
```{r, echo = FALSE}
# Duration of training before withdrawal in weeks
duration <- dc2 %>% filter(Working == "Fail") %>% select(Duration)/7
print('Duration of Training in weeks')
summary(duration)

h <- ggplot(duration, aes(x=Duration)) +
 geom_histogram(binwidth = 1) +
 xlab("Duration (Weeks)") +
 ylab("Number of Dogs") + theme_bw()
h

fig_results(h, name = "duration-histogram",
            caption = "Duration of training in weeks for dogs that were withdrawn from training for behavioural reasons.")
```

## Descriptive Statistics

Calculate descriptive statistics for numerical and categorical variables.
```{r, echo = FALSE}
# NUMERICAL
stats_num <- data.frame(do.call(rbind, lapply(dc2 %>% select(where(is.numeric)), summary)))
stats_num$NA.s <- colSums(is.na(dc2 %>% select(where(is.numeric))))
stats_num
tab_results(stats_num, name = "descriptive-statistics-numerical")

# CATEGORICAL
stats_cat <- data.frame(do.call(rbind, lapply(dc2 %>% select(all_of(cat), -Kong.Interaction.Lateralisation), summary)))
stats_cat$NA.s <- colSums(is.na(dc2 %>% select(all_of(cat), -Kong.Interaction.Lateralisation)))
stats_cat
tab_results(stats_cat, name = "descriptive-statistics-categorical")

tab_results(table(dc2$Kong.Interaction.Lateralisation), name = 'descriptive-kong-lateralisation')
```

## Validity
Categorical with p<0.25: 2 out of 17 ethogram items, 0 out of 2 demographics variables (Sex and Breed).
```{r, echo = FALSE}
# CATEGORICAL: univariate logistic regression using one predictor and outcome
dc_cat <- dc2 %>% select(all_of(cat), -c(Isolation.Urinating, Crate.Urinating, 
                Body.check.General..Mouths.), Sex, Breed, Working)
dc_cat_lr <- lapply(dc_cat[-length(dc_cat)], 
                        function(x) glm(formula = Working ~ x, 
                        data = dc_cat, 
                        family = binomial(link = logit), na.action = na.exclude))
# results in list                       
dc_cat_res <- lapply(dc_cat_lr, function(x) c(coef(summary(x)), summary(x)$deviance))

# 2 factors -> add to dataframe
dc_cat_res <- data.frame(do.call(rbind, dc_cat_res[lapply(dc_cat_res, length) == 9]))

# 3 factors
cat("Categorical variable with 3 factors:", levels(dc2$Breed))
print(summary(dc_cat_lr[16]$Breed))
tab_results(model_or(dc_cat_lr[16]$Breed), name = "breed")

# 4 factors
cat("Categorical variable with 4 factors:", levels(dc2$Kong.Interaction.Lateralisation))
print(summary(dc_cat_lr[14]$Kong.Interaction.Lateralisation))
tab_results(model_or(dc_cat_lr[14]$Kong.Interaction.Lateralisation), name = "lateralisation")
```

Ordinal with p<0.25: 16 out of 53 ethogram items. Score: 14 out of 42 variables.
```{r, echo = FALSE}
# ORDINAL: univariate logistic regression using one predictor and outcome
dc_ord <- dc2 %>% select(all_of(ord), matches("^S[.]"), Working)
dc_ord_lr <- lapply(dc_ord[-length(dc_ord)],
                      function(x) glm(formula = Working ~ x, 
                      data = dc_ord,
                      family = binomial(link = logit), na.action = na.exclude))

# results in dataframe
dc_ord_res <- lapply(dc_ord_lr, function(x) round(c(coef(summary(x)), summary(x)$deviance), 4))
dc_ord_res <- data.frame(do.call(rbind, dc_ord_res))
```

Positive estimates increase the probability of failure, while negative estimates decrease the probability of failure.
Save results from univariate analysis.
```{r, echo = FALSE}
# saving univariate logistic regression results to csv
dc_lr <- rbind(dc_ord_res, dc_cat_res)
colnames(dc_lr) <- c("estimate_0", "estimate_1", "se_0", "se_1", "z_value_0", "z_value_1",
                    "p_value_0", "p_value_1", "deviance")          
tab_results(dc_lr, name = "univariate-logistic-regression")
```

Converting logistic regression result to odds ratio and confidence intervals. 
```{r, echo = FALSE}
lr <- dc_lr %>% select(estimate_1, p_value_1)
colnames(lr) <- c("Estimate", "p_value")
# create odds ratio and level of significance
lr$OR <- exp(lr$Estimate)
lr <- lr %>% mutate(Significance = case_when(
            (p_value > 0.10 & p_value < 0.15) ~ "*",
            (p_value >0.05 & p_value < 0.10) ~ "**",
            p_value < 0.05 ~ "***"), .before = Estimate)

# list of model results ORDINAL + CATEGORICAL
lr_cis <- lapply(c(dc_ord_lr, dc_cat_lr[c(-14, -16)]), # removing lateralisation and breed
                function(x) as.data.frame(exp(confint(x, level = 0.95))))
# list of dataframes removing the CI for the intercept
lr_cis <- lapply(lr_cis, function(x) x[2,])
# turn list of dataframes into a dataframe
lr_ci <- bind_rows(lr_cis, .id = 'x')
# replace rownames with column with the name of the variables
lr_ci <- lr_ci %>% remove_rownames %>% column_to_rownames(var="x") %>% 
                mutate_if(is.numeric, round, 2)
# bind CIs
lr <- cbind(lr, lr_ci)

# save csv and print latex code
tab_results(lr %>% mutate_if(is.numeric, round, 2), name = "lr",
    caption = "Univariate logistic regression model estimates, p-value, odds ratio (OR) and confidence interval (CI)")
```

## Feature Selection
Metholodogy adapted from Hosmer2013, pg 107 (GLOW study example).

### Step 1: Univariate logistic regression model results. 
Features which achieved P < 0.10 were selected for consideration to be included in the reduced feature set. 
Subsequentely, related variables were removed considering two cases: 
- Two scores: when both the mean and product scores were selected, the score with the highest p-value was removed.
- Score and original items: when both the original item and the score derived from it were selected, the one with the highest p-value was removed.

Variables whose logistic regression model resulted in p<0.25 when used to predict Working outcome are shown.
**Variables with p < 0.25 (N = 32): original (N = 18), scores (N = 14) some are repeated**, 
**after removing related variables: variables (N = 14): original (N = 7), scores (N = 7)**.

```{r, echo = FALSE}
# with two breeds only!
# # p<0.05 + scores 
# data_1 <- dc2 %>% select(Working,Kong.Interaction.Lateralisation,  S.Walking.Pull_mean, Distractions.First.Response..Car., Kong.Interaction.Response.to.stimulus, 
#             Kong.Return.Handler, S.Crate.Handler_mean, S.Isolation.Handler_mean, S.Isolation.Stimulus ,
#             S.Sensitivity, Tea.Towel.Second.Response..Indifferent., Tea.Towel.Second.Response..Change.from.Neutral.)

# # p<0.10 + scores (pvalue)
# data_1 <- dc2 %>% select(Working, Kong.Interaction.Lateralisation, S.Walking.Pull_mean, S.Sensitivity, Crate.Behaviours..Actively.Seeking.Attention.,
#             Distractions.First.Response..Car., Kong.Interaction.Response.to.stimulus, Kong.Return.Handler, S.Isolation.Stimulus, 
#             Tea.Towel.Second.Response..Indifferent., Tea.Towel.Second.Response..Change.from.Neutral.)
            

# # p<0.20 + scores (pvalue),
# data_1 <- dc2 %>% select(Working, Kong.Interaction.Lateralisation, S.Walking.Pull_mean, S.Sensitivity, S.Kong.Response_prod, 
#             S.Crate.Handler_mean, S.Isolation.Stimulus, S.Isolation.Handler_mean, S.Familiarisation.Handler_mean, 
#             Tea.Towel.Second.Response..Indifferent., Tea.Towel.Second.Response..Change.from.Neutral., 
#             Distractions.First.Response..Car., Distractions.Second.Response..Car., Standing.Response, Tea.Towel.Second.Response..Attempts.to.Removes.towel.with.mouth.) #  maybe without GR

# # p<0.10 + pvalue
# data_1 <- dc2 %>% select(Kong.Interaction.Lateralisation, Working, Kong.Interaction.Response.to.stimulus, Distractions.First.Response..Car., 
#             S.Walking.Distractions.Pull_mean, Kong.Interaction.Response.to.stimulus, Kong.Return.Handler, S.Isolation.Stimulus, 
#             Tea.Towel.Second.Response..Indifferent., Tea.Towel.Second.Response..Change.from.Neutral.)
# data_1 %>% colnames()

# # visualise pvalues of the variables selected for next step 
# dc_lr %>% filter(row.names(dc_lr) %in% colnames(data_1)) %>% select(p_value_1) %>% arrange(desc(p_value_1)) 
```

```{r, echo = FALSE}
# sort features by p values from univariate analysis
colnames(lr)
lr %>% filter(p_value < 0.05) %>% select(Estimate, p_value)
lr %>% filter(p_value > 0.05 & p_value < 0.10) %>% nrow()

# p<0.15 + scores (pvalue)Kong.Interaction.Lateralisation,
data_1 <- dc2 %>% select(Working, S.Walking.Pull_mean, S.Sensitivity, S.Kong.Response_prod,
            S.Crate.Handler_mean, S.Isolation.Stimulus, S.Isolation.Handler_mean, S.Familiarisation.Handler_mean,
            Tea.Towel.Second.Response..Indifferent., Tea.Towel.Second.Response..Change.from.Neutral.,
            Distractions.First.Response..Car.) # Distractions.Second.Response..Car. maybe without GR

# print pvalue of selected features
# data_1 %>% colnames()
# dc_lr %>% filter(row.names(dc_lr) %in% colnames(data_1)) %>% select(p_value_1)
```

### Step 2: Correlation
```{r, echo = FALSE}
# calculate correlation for numeric variables
correlationMatrix <- round(abs(cor(data_1 %>% select(where(is.numeric)), 
                            method = "kendall",
                            use = "pairwise.complete.obs")),2)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.4, names = TRUE, exact = TRUE)
print(highlyCorrelated)

plot <- ggplot(data = melt(correlationMatrix), aes(x=Var1, y=Var2, fill=value)) + 
        geom_tile() + xlab("Variables") + ylab(NULL) + 
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5), 
        axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))
        
plot

fig_results(plot = plot, name = "correlation", 
        caption = "Correlation matrix for variables considered for the reduced feature set")

# calculate correlation for factors variables
chisq.test(data_1$Tea.Towel.Second.Response..Indifferent., data_1$Tea.Towel.Second.Response..Change.from.Neutral.)
# chisq.test(data_1$Tea.Towel.Second.Response..Indifferent., data_1$Tea.Towel.Second.Response..Attempts.to.Removes.towel.with.mouth.)

# remove highly correlated features
data_2 <- data_1 %>% select(-all_of(highlyCorrelated), -Tea.Towel.Second.Response..Change.from.Neutral.)#, -Tea.Towel.Second.Response..Attempts.to.Removes.towel.with.mouth.)
data_reduced <- data_2
```

## Missing Data
dc2, missing values per column and per row. Very few data are missing considering the original items selected.
```{r, echo = FALSE}
# reduced dataset
dc_col_nas <- unlist(lapply(data_reduced, function (x) sum(is.na(x))))
print(dc_col_nas)
dc_row_nas <- apply(data_reduced, 1, function (x) sum(is.na(x)) ) 
table(dc_row_nas)
```

## Visualization

<!-- I think for the factors, because they are continuous variables (average of discrete variables), it is better to only have the boxplot. -->

```{r, echo = FALSE}
#categorical
cat_feat <- data_reduced %>% select(where(is.factor), -Working) %>% colnames()

# geom_mosaic won't work properly with aes_string
cat("Plot these features manually: ", cat_feat)

var = "Tea.Towel.Second.Response..Indifferent."
plot1 <- ggplot(data_reduced, aes_string( x = var, fill = 'Working', width = 1)) + 
        geom_bar(position = "dodge") +
        theme(legend.position = c(0.85,0.88), axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))

plot2 <- ggplot(data = data_reduced) +
        geom_mosaic(aes(x=product(Tea.Towel.Second.Response..Indifferent., Working), fill = Working), alpha = 0.5) +
        theme(legend.position = "none", axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))

plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

title <- ggdraw() + 
    draw_label(gsub("[.]", " ", var), fontface = 'bold')

plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))
                
fig_results(plot = plot3,
                name = gsub("[.]", "-", var),
                caption = gsub("[.]", " ", var),
                label = unlist(strsplit(var, "[.]"))[1])
plot
```

```{r, echo = FALSE}
# numerical
num_feat <- data_reduced %>% select(where(is.numeric))  %>% colnames()

# discrete
plot_discrete <- function(var){
    
    plot1 <- ggplot(data_reduced, aes_string( x = var, fill = 'Working', width = 1)) + 
            geom_bar(position = "dodge") +
            theme(legend.position = c(0.84,0.88), axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20)) 

    plot2 <- ggplot(data_reduced, aes_string( x = 'Working', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Working, fill = Working), alpha = 0.2)  +
            geom_boxplot(position = "dodge", aes(colour = Working), fill = "white",  width = 0.3) +
            ylab("Rating") +
            theme(legend.position = "none", axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
      draw_label(gsub("[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    fig_results(plot = plot3,
                name = gsub("[.]", "-", var),
                caption = gsub("[.]", " ", var),
                label = unlist(strsplit(var, "[.]"))[1])
    plot
}

disc_feat <- num_feat[!grepl("^S[.]", num_feat)]
lapply(disc_feat, plot_discrete)

# continuous
plot_continuous <- function(var){

    bin = ceiling((max(data_reduced[var], na.rm = TRUE) - min(data_reduced[var], na.rm = TRUE)) / 5)
    # cat(var, bin, "\n") 
    
    plot1 <- ggplot(data_reduced, aes_string(x = var, fill = 'Working')) + 
            geom_histogram(binwidth = bin, position = "dodge") +
            theme(legend.position = c(0.84,0.88), axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))

    plot2 <- ggplot(data_reduced, aes_string( x = 'Working', y = var)) + 
            geom_violin(position = "dodge", width = 0.8, aes(color = Working, fill = Working), alpha = 0.2) +
            theme(legend.position = "none",axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20)) +
            geom_boxplot(position = "dodge", aes(colour = Working), fill = "white",  width = 0.3) +
            ylab("Rating")
    plot3 <- plot_grid(plot1, plot2, rel_widths = c(2, 1), labels = "AUTO")

    title <- ggdraw() + 
      draw_label(gsub("S[.]|[.]", " ", var), fontface = 'bold')
    
    plot <- plot_grid(title, plot3, ncol = 1, rel_heights = c(0.1, 1))

    fig_results(plot = plot3,
                name = gsub("[.]", "-", var),
                caption = gsub("[.]", " ", var),
                label = unlist(strsplit(var, "[.]"))[1])

    plot
}

cont_feat <- num_feat[grepl("^S[.]", num_feat)]
lapply(cont_feat, plot_continuous)
```

# Multivariate Model

Preprocessing: Normalization + Imputation. Recursive Feature Elimination (RFE) + Multivariate Logistic Regression (LR).
Build model.
```{r, echo = FALSE}
# columns used as predictors
# data_reduced <- data_1
data_reduced %>% colnames()

# GLM predicts the SECOND class -> make sure Fail is second class
# data_reduced$Working <- relevel(data_reduced$Working, ref = "Success")
# RFE calculates the metrics on the FIRST class -> make sure Fail is first class
data_reduced$Working <- relevel(data_reduced$Working, ref = "Fail")
cat('predicting second class:', levels(data_reduced$Working))

model_recipe <- recipe(Working ~ ., data = data_reduced )  %>%
                    step_dummy(all_nominal(), -Working) %>% 
                    step_impute_mean(all_numeric()) %>%
                    step_normalize(all_numeric()) 

# Setting ROC as the metric for the Logistic Regression function
# lrFuncs$summary <- twoClassSummary 
lrFuncs$summary <- prSummary 
set.seed(42)

ctrl <- rfeControl(functions = lrFuncs,         # Logistic Regression
                method = "cv",                  # Cross Validation
                number = nrow(data_reduced),    # Number of folds
                saveDetails = TRUE,
                returnResamp = "all",
                allowParallel = FALSE,
                rerank = TRUE,
                verbose = FALSE)

# Recursive Feature Elimination with feat_lr
model_rfe <- rfe(model_recipe,                 # predict Working using all other variables
                data = data_reduced,           # selecting the features from univariate lr
                sizes = 1:(ncol(data_reduced)-1),# from 1 to the number of features in the dataset
                rfeControl = ctrl,
                metric = "Precision",
                maximize = TRUE)

warnings()
print(model_rfe)
model <- summary(model_rfe$fit)
print(model)
# print(model_rfe$optVariables)
```

Model fitness with deviance analysis. 
```{r, echo = FALSE}
# model fitness
dev = model$null.deviance - model$deviance
deg = model$df.null - model$df.residual
cat('\ndeviance difference: ', dev)
cat('\ndf difference: ', deg)
cat('\nlevel of significance: ', pchisq(dev, deg, lower.tail = FALSE))
cat('\nthe model is a good fit: ', pchisq(dev, deg, lower.tail = FALSE) < 0.05)
```

## Performance
Performance metrics ROC and PR curves. 
```{r, echo = FALSE}
# cross validated predictions for performance analysis
df_perf <- model_rfe$pred %>% 
            filter(Variables == model_rfe$optsize) %>% # get results from optimum model  
            select(rowIndex, Fail, pred, obs) %>%
            rename(c("Index" = "rowIndex", "Probability" = "Fail", "Predicted" = "pred", "Working" = "obs"))


roc_curve <- roc.curve(scores.class0 = df_perf$Probability,
             weights.class0 = abs(as.numeric(df_perf$Working)-2),
             curve=TRUE)
plot(roc_curve)
png('3_results/dc2-working/roc-curve.png')
plot(roc_curve, xlab = "1 - Specificity", auc.main = FALSE, col = 'blue',
         main = paste('ROC Curve (AUC = ',toString(round(roc_curve$auc,2)),')',sep = ''), 
         cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
dev.off()

pr_curve <- pr.curve(scores.class0 = df_perf$Probability,
             weights.class0 = abs(as.numeric(df_perf$Working)-2),
             curve=TRUE)
plot(pr_curve)
png('3_results/dc2-working/pr-curve.png')
plot(pr_curve, auc.main = FALSE, col = 'blue',
         main = paste('PR Curve (AUC = ',toString(round(pr_curve$auc.integral,2)),')',sep = ''), 
         cex.lab=1.5, cex.axis=1.5, cex.main=1.5, cex.sub=1.5)
dev.off()

# calculate performance metrics
cm <- confusionMatrix(data = df_perf$Predicted, reference = df_perf$Working, positive = "Fail")

metrics <- data.frame(Accuracy = cm$overall['Accuracy'], 
                        ROC = roc_curve$auc,
                        PR =  pr_curve$auc.integral, 
                        t(cm$byClass))

metrics %>% round(3) %>% select(Accuracy, ROC, PR, Precision, Recall, Sensitivity, Specificity, F1, Balanced.Accuracy)
```

Plot the logit.
```{r, echo = FALSE}
# logit function, calculate y given the probability(x)
y <- function(x){ return(log(1/( 1/x - 1 ))) }
df_perf$Estimate <- y(df_perf$Probability)
df_perf$Status <- dc2$Status

# releveling for the colours i the plot
df_perf$Working <- relevel(df_perf$Working, "Fail")
df_perf$Status <- relevel(df_perf$Status, "W")

# plot Logit vs Working
print("Probability predicted by the LR using PRQ data vs true Working")
plot <- ggplot(df_perf, aes(x = Estimate, y = Probability, group = Working ))  +
    theme(legend.position = c(0.14,0.88), axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))+
    geom_point(aes(shape=Working, color=Working), size = 5)
plot

fig_results(plot = plot, name = 'logit-outcome', 
            caption = "Probability predicted by the best logistic regression model using \acrshort{cbarq} data vs true Working")

# plot Logit vs Status
print("Probability predicted by the LR using PRQ data vs true Status")
plot <- ggplot(df_perf, aes(x = Estimate, y = Probability, color = Status )) +
    theme(legend.position = c(0.14,0.88), axis.title=element_text(size=20,face="bold"), axis.text=element_text(size=20), text = element_text(size = 20))+
    geom_point(aes(shape=Status, color=Status), size = 5)
plot

fig_results(plot = plot, name = 'logit-status', 
            caption = "Probability predicted by the best logistic regression model using \acrshort{cbarq} data vs true Status")

print("Estimate and Probabilities per training outcome status")
df_perf %>% group_by(Status) %>% dplyr::summarize(Mean = mean(Estimate, na.rm=TRUE))
df_perf %>% group_by(Status) %>% dplyr::summarize(Mean = mean(Probability, na.rm=TRUE))
```

Check the dogs that were false positives or false negatives
```{r, echo = FALSE}
# FALSE POSITIVE dogs predicted to fail, but succeed
# df_perf %>% filter(Predicted == "Fail" & Working == "Success") %>% arrange(Probability) %>% select(Index, Probability)
dc2$Name[df_perf %>% filter(Predicted == "Fail" & Working == "Success") %>% arrange(Probability) %>% pull(Index)]
# FALSE NEGATIVE dogs predicted to succeed, but failed
# df_perf %>% filter(Predicted == "Success" & Working == "Fail") %>% arrange(Probability) %>% select(Index, Probability)
dc2$Name[df_perf %>% filter(Predicted == "Success" & Working == "Fail") %>% arrange(Probability) %>% pull(Index)]
```

## Interpretation
Discuss model interpretation.
```{r, echo = FALSE}
# the glm was predicts probability of "Success", so I needed to change the signs here
results_rfe <- as.data.frame(coef(summary(model_rfe$fit))) %>% mutate(Estimate  = -Estimate)
colnames(results_rfe)[4] <- "P-value"
results_rfe$OR <- exp(results_rfe$Estimate)
results_rfe <- cbind(results_rfe, exp(-confint(model_rfe$fit, level = 0.95)))
results_rfe <- results_rfe %>% select("Estimate", "P-value", "OR", "2.5 %"="97.5 %","97.5 %" = "2.5 %")
print(results_rfe)

# save csv and print latex code
tab_results(results_rfe,
    caption = "Optimal multivariate logistic regression model selected by \acrshort{rfe}",
    name = "rfe-lr")
```


Descriptive statistics and univariate models for the features of the optimal model.
```{r, echo = FALSE}
tab_results(stats_num %>% filter(row.names(stats_num) %in% model_rfe$optVariables), name = "descriptive-optimal-variables")
tab_results(lr %>% filter(row.names(lr) %in% model_rfe$optVariables), name = "lr-optimal-variables")
```

Variable importance of logistic regression models is calculated based on the absolute value of the t-statistic.
```{r, echo = FALSE}
tab_results(varImp(model_rfe$fit), caption = "Importance of features in the optimal model", name = "feature-importance")
```
# Conclusion